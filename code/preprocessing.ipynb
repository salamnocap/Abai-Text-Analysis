{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "import os, re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter1 = ['the_home_coming', 'the_whirlwind', 'on_the_road', 'in_the_thickets', 'along_the_foothills', 'the_ascent', 'in_the_heights']\n",
    "chapter2 = ['the_ford', 'the_foothills', 'over_the_ruts', 'through_the_pass', 'at_the_crossroads', 'at_the_summit', 'epilogue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokenized_text = []\n",
    "    for sentence in sent_tokenize(text):\n",
    "        cleaned_sentence = re.sub(r'[^a-zA-Z\\s]', '', sentence)\n",
    "        tokenized_text.append(list(word_tokenize(cleaned_sentence)))\n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chapters_to_json(dir: str):\n",
    "    dict = {}\n",
    "    with open(f'{dir}/foreword.txt', 'r') as f:\n",
    "        dict['foreword'] = tokenize_text(f.read())\n",
    "\n",
    "    chapter1_dict = {}\n",
    "    for chapter in chapter1:\n",
    "        with open(f'{dir}/{chapter}.txt', 'r') as f:\n",
    "            chapter1_dict[chapter] = tokenize_text(f.read())\n",
    "    dict['part1'] = chapter1_dict\n",
    "\n",
    "    chapter2_dict = {}\n",
    "    for chapter in chapter2:\n",
    "        with open(f'{dir}/{chapter}.txt', 'r') as f:\n",
    "            chapter2_dict[chapter] = tokenize_text(f.read())\n",
    "    dict['part2'] = chapter2_dict\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters_json = chapters_to_json('en')\n",
    "with open('data.json', 'w') as f:\n",
    "    f.write(str(chapters_json))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KAZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text_kaz(text):\n",
    "    tokenized_text = []\n",
    "    for sentence in sent_tokenize(text):\n",
    "        cleaned_sentence = re.sub(r'[^а-яА-ЯәӘғҒқҚңҢөӨұҰүҮіІ\\s]', '', sentence)\n",
    "        tokenized_text.append(list(word_tokenize(cleaned_sentence)))\n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "part1 = {\n",
    "    'kaitkanda': 'қайтқанда',\n",
    "    'kat_kabatta': 'қат-қабатта',\n",
    "    'zholda': 'жолда',\n",
    "    'shytyrmanda': 'шытырманда',\n",
    "    'bel_beleste': 'бел-белесте',\n",
    "    'orde': 'өрде',\n",
    "    'kyiada': 'қияда'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "part2 = {\n",
    "    'taigakta': 'тайғақта', \n",
    "    'zhailauda': 'жайлауда', \n",
    "    'enyste': 'еңісте', \n",
    "    'okapta': 'оқапта', \n",
    "    'asuda': 'асуда', \n",
    "    'tarauda': 'тарауда', \n",
    "    'biykte': 'биікте', \n",
    "    'epilogue': 'эпилог'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "part3 = {\n",
    "    'abai_aga': 'абай аға',\n",
    "    'kek_zholynda': 'кек жолында',\n",
    "    'karashygyn': 'қарашығын',\n",
    "    'okinishte': 'өкініште',\n",
    "    'kaktygysta': 'қақтығыста',\n",
    "    'korshauda': 'қоршауда'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "part4 = {\n",
    "    'tun_tunekte': 'түн-түнекте',\n",
    "    'kuz_kiyada': 'құз-қияда',\n",
    "    'kapada': 'қапада',\n",
    "    'kastykta': 'қастықта',\n",
    "    'shaikasta': 'шайқаста',\n",
    "    'zhutta': 'жұтта',\n",
    "    'epilogue': 'эпилог'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_files(path,part):\n",
    "    for name in part:\n",
    "        with open(f\"{path}/{name}.txt\", 'w') as f:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_files('kaz/part3', part3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chapters_to_json(dir: str, part1, part2, part3, part4):\n",
    "    dict = {}\n",
    "    \n",
    "    dict['chapters_in_kaz'] = {\n",
    "        'part1': part1,\n",
    "        'part2': part2,\n",
    "        'part3': part3,\n",
    "        'part4': part4\n",
    "    }\n",
    "\n",
    "    part1_dict = {}\n",
    "    for chapter in part1:\n",
    "        with open(f'{dir}/part1/{chapter}.txt', 'r') as f:\n",
    "            part1_dict[chapter] = tokenize_text_kaz(f.read())\n",
    "    dict['part1'] = part1_dict\n",
    "\n",
    "    part2_dict = {}\n",
    "    for chapter in part2:\n",
    "        with open(f'{dir}/part2/{chapter}.txt', 'r') as f:\n",
    "            part1_dict[chapter] = tokenize_text_kaz(f.read())\n",
    "    dict['part2'] = part2_dict\n",
    "\n",
    "    part3_dict = {}\n",
    "    for chapter in part3:\n",
    "        with open(f'{dir}/part3/{chapter}.txt', 'r') as f:\n",
    "            part1_dict[chapter] = tokenize_text_kaz(f.read())\n",
    "    dict['part3'] = part3_dict\n",
    "\n",
    "    part4_dict = {}\n",
    "    for chapter in part4:\n",
    "        with open(f'{dir}/part4/{chapter}.txt', 'r') as f:\n",
    "            part4_dict[chapter] = tokenize_text_kaz(f.read())\n",
    "    dict['part4'] = part4_dict\n",
    "\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaz = chapters_to_json('kaz', part1, part2, part3, part4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('kaz/kaz.json', 'w') as f:\n",
    "    f.write(str(kaz))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
